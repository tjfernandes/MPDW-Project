





from PIL import Image
import requests
import torch
import pprint as pp


from transformers import CLIPProcessor, CLIPModel





device = "cuda:0" if torch.cuda.is_available() else "cpu"

print('Working on device: ', device)

# Load the pre-trained CLIP model and processor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)


url_chicken_parm = "https://m.media-amazon.com/images/S/alexa-kitchen-msa-na-prod/recipes/thekitchn/016aa4923f044e1bad4cb2802f04133f7cf787b9bbf4fceb52438ecb70b28d89.jpg"
image = Image.open(requests.get(url_chicken_parm, stream=True).raw)
input_encoding = processor(
    text=["a photo of a cuba libre", "a photo of chicken and parmesan cheese"],
    images=image,
    return_tensors="pt",
    padding=True
).to(device)





input_encoding.keys()


input_encoding['input_ids']


pp.pprint(processor.tokenizer.convert_ids_to_tokens(input_encoding["input_ids"][0]))
pp.pprint(processor.tokenizer.convert_ids_to_tokens(input_encoding["input_ids"][1]))


print(processor.tokenizer.decode(input_encoding["input_ids"][0].tolist()))
print(processor.tokenizer.decode(input_encoding["input_ids"][1].tolist()))





input_encoding['pixel_values'].squeeze().size()


from IPython.display import Image
Image(url_chicken_parm)





outputs = model(**input_encoding)
logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities
probs








input_encoding = processor(
    text=["a photo of a cuba libre", "a photo of chicken and parmesan cheese"], 
    return_tensors="pt", 
    padding=True
).to(device)
text_embeddings = model.get_text_features(**input_encoding)
text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)
text_embeddings.size()





# The output image embedding is a 512 dimensional vector
input_img = processor(images=image, return_tensors="pt").to(device)
image_embeddings = model.get_image_features(**input_img)
image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
image_embeddings.size()






image_embeddings @ text_embeddings.T





from PIL import Image
import requests
import torch

from transformers import CLIPProcessor, CLIPModel

device = "cuda:0" if torch.cuda.is_available() else "cpu"

# Load the pre-trained CLIP model and processor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)

image_urls = [
    "https://m.media-amazon.com/images/S/alexa-kitchen-msa-na-prod/recipes/thekitchn/016aa4923f044e1bad4cb2802f04133f7cf787b9bbf4fceb52438ecb70b28d89.jpg",
    "https://m.media-amazon.com/images/S/alexa-kitchen-msa-na-prod/recipes/thekitchn/2eba4355b22b630a2a230184619a07db3658962bae1a830cbc2c1cc9e93e86eb.jpg",
    "https://m.media-amazon.com/images/S/alexa-kitchen-msa-na-prod/recipes/thekitchn/77d73112c0675e4107d9c0e0dd5ee1038e43a8cb495ff2d8aeebc77564f6089d.jpg",
]

# Load all images
images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]

# Encode images
image_inputs = processor(images=images, return_tensors="pt", padding=True).to(device)
with torch.no_grad():
    image_embeddings = model.get_image_features(**image_inputs)



# Encode text query
text_inputs = processor(text=["a photo of pesto"], return_tensors="pt").to(device)
with torch.no_grad():
    text_embeddings = model.get_text_features(**text_inputs)
    
    
# Calculate similarity
similarity = text_embeddings @ image_embeddings.T
values, indices = torch.topk(similarity, 1)

# Return the most similar image
most_similar_image = images[indices[0]]



# Load all descriptions
descriptions = ["a chicken and parmesan great dish", "pesto sauce, delicious", "amazing corn tortillas for you"]

# Encode descriptions
text_inputs = processor(text=descriptions, return_tensors="pt", padding=True).to(device)
with torch.no_grad():
    text_embeddings = model.get_text_features(**text_inputs)


image_url = "https://m.media-amazon.com/images/S/alexa-kitchen-msa-na-prod/recipes/thekitchn/77d73112c0675e4107d9c0e0dd5ee1038e43a8cb495ff2d8aeebc77564f6089d.jpg"
# Load and encode image
image = Image.open(requests.get(image_url, stream=True).raw)
image_input = processor(images=image, return_tensors="pt").to(device)
with torch.no_grad():
    image_embeddings = model.get_image_features(**image_input)


# Calculate similarity
similarity_descr = image_embeddings @ text_embeddings.T
values_descr, indices_descr = torch.topk(similarity_descr, 1)

# Return the most similar description_descr
most_similar_description = descriptions[indices_descr[0].item()]



from IPython.display import display
# Display the image
display(most_similar_image)


print(most_similar_description)



