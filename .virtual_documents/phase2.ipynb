





import json as json
    
with open("recipes_data.json", "r") as read_file:
    recipes_data = json.load(read_file)


recipe_book_len = len(recipes_data)
str(recipe_book_len)











import pprint as pp
import requests
from config import CONFIG

host = CONFIG["host"]
port = CONFIG["port"]
user = CONFIG["user"]
password = CONFIG["password"]
#index_name = CONFIG["index_name"]
index_name = user





from opensearchpy import OpenSearch
from opensearchpy import helpers

# Create the client with SSL/TLS enabled, but hostname verification disabled.
client = OpenSearch(
    hosts = [{'host': host, 'port': port}],
    http_compress = True, # enables gzip compression for request bodies
    http_auth = (user, password),
    url_prefix = 'opensearch',
    use_ssl = True,
    verify_certs = False,
    ssl_assert_hostname = False,
    ssl_show_warn = False
)





resp = client.indices.close(index = index_name, timeout=600)
print(resp)





#be absolutely sure that you want to comment this line and actually delete the index!!!

if client.indices.exists(index=index_name):
    # Delete the index.
    response = client.indices.delete(
        index = index_name,
        timeout = 600
    )
    print('\nDeleting index:')
    print(response)






index_body = {
    "settings":{
        "index":{
            "number_of_replicas":0,
            "number_of_shards":4,
            "refresh_interval":"1s",
            "knn":"true"
        }
    },
    "mappings": {
            "properties": {
                "recipe_id": {"type": "keyword"},
                "title": {"type": "text"},
                "description": {"type": "text"},
                "time": {"type": "integer"},
                "difficulty": {"type": "keyword"},
                "ingredients": {
                    "type": "nested",
                    "properties": {
                        "text": {"type": "text"},
                        "name": {"type": "text"},
                        "quantity": {"type": "float"},
                        "unit": {"type": "keyword"}
                    }
                },
                "instructions": {
                    "type": "nested",
                    "properties": {
                        "stepNumber": {"type": "integer"},
                        "text": {"type": "text"},
                        "durationSeconds": {"type": "integer"}
                    }
                },
                "nutrients": {
                    "type": "object",
                    "properties": {
                        "calories": {
                            "type": "object",
                            "properties": {
                                "quantity": {"type": "float"},
                                "measurement": {"type": "keyword"}
                            }
                        },
                        "protein": {
                            "type": "object",
                            "properties": {
                                "quantity": {"type": "float"},
                                "measurement": {"type": "keyword"}
                            }
                        },
                        "fat": {
                            "type": "object",
                            "properties": {
                                "quantity": {"type": "float"},
                                "measurement": {"type": "keyword"}
                            }
                        },
                        "carbohydrates": {
                            "type": "object",
                            "properties": {
                                "quantity": {"type": "float"},
                                "measurement": {"type": "keyword"}
                            }
                        }
                    }
                },
                "title_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                        "ef_construction":256,
                        "m":48
                        }
                    }
                },
                "description_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                        "ef_construction":256,
                        "m":48
                        }
                    }
                },
                "time_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                          "ef_construction":256,
                          "m":48
                        }
                    }
                },
                "difficulty_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                          "ef_construction":256,
                          "m":48
                        }
                    }
                },
                "ingredients_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                          "ef_construction":256,
                          "m":48
                        }
                    }
                },
                "instructions_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                          "ef_construction":256,
                          "m":48
                        }
                    }
                },
                "nutrients_embedding":{
                    "type":"knn_vector",
                    "dimension": 768,
                    "method":{
                        "name":"hnsw",
                        "space_type":"innerproduct",
                        "engine":"faiss",
                        "parameters":{
                          "ef_construction":256,
                          "m":48
                        }
                    }
                },
                "images_embedding":  {"type": "binary"}
            }
        }
    }

if client.indices.exists(index=index_name):
    print("Index already existed. Nothing to be done.")
else:        
    response = client.indices.create(index_name, body=index_body)
    print('\nCreating index:')
    print(response)









from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel

import torch
import torch.nn.functional as F
import pickle
import os
from PIL import Image
import requests
import json


#Mean Pooling - Take average of all tokens
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


#Encode text
def encode(texts):
    # Tokenize sentences
    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

    # Compute token embeddings
    with torch.no_grad():
        model_output = model(**encoded_input, return_dict=True)

    # Perform pooling
    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

    # Normalize embeddings
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    return embeddings      

def encode_images(image_urls):
    # Load the pre-trained CLIP model and processor
    
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    
    # Load all images
    images = [Image.open(requests.get(url, stream=True).raw).resize((224, 224)) for url in image_urls]

    # Encode images
    image_inputs = processor(images=images, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        image_embeddings = model.get_image_features(**image_inputs)
    
    return image_embeddings

def encode_images_in_batches(images, batch_size=32):
        for i in range(0, len(images), batch_size):
            yield encode_images(images[i:i+batch_size])
        
def get_embeddings():
    if os.path.exists('embeddings.pkl'):
        # Load embeddings from a file
        print('Embeddings file found. Loading embeddings...')
        with open('embeddings.pkl', 'rb') as f:
            return pickle.load(f)
    else:
        print('Embeddings file not found. Generating embeddings...')
        return add_embeddings()
    
def add_embeddings():
    titles = []
    descriptions = []
    times = []
    difficulties = []
    # ingredients = []
    # instructions = []
    images = []
    
    for recipe in recipes_data.values():
        titles.append(recipe["displayName"] if recipe["displayName"] is not None else 'None') 
        descriptions.append(recipe["description"] if recipe["description"] is not None else 'None')
        #times.append(str(recipe["totalTimeMinutes"]) if recipe["totalTimeMinutes"] is not None else 'None')        
        #difficulties.append(recipe["difficultyLevel"] if recipe["difficultyLevel"] is not None else 'None')
        # for ingredient in recipe["ingredients"]:
        #     ingredients.append(ingredient if ingredient is not None else 'None')
        # for instruction in recipe["instructions"]:
        #     instructions.append(instruction["stepText"] if instruction["stepText"] is not None else 'None')
        for image in recipe["images"]:
            images.append(image["url"] if image["url"] is not None else 'None')
        
    # Calculate embeddings
    titles_emb = encode(titles)
    descriptions_emb = encode(descriptions)
    #times_emb = encode(times)
    #difficulties_emb = encode(difficulties)
    # ingredients_emb = encode(ingredients)
    # instructions_emb = encode(instructions)

    images_emb_generator = encode_images_in_batches(images)

    all_images_emb = []

    # Save embeddings to a file
    try:
        for images_emb in images_emb_generator:
            # Append each batch of image embeddings to the list
            all_images_emb.extend(images_emb)
            
        with open('embeddings.pkl', 'wb') as f:
            pickle.dump({
                'titles': titles_emb,
                'descriptions': descriptions_emb,
                # 'times': times_emb,
                # 'difficulties': difficulties_emb,
                # 'ingredients': ingredients_emb,
                # 'instructions': instructions_emb,
                'images': all_images_emb
            }, f)
    except Exception as e:
        print(f"Error while writing embeddings to file: {e}")
        return None
            
    # Free up GPU memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
    # Load embeddings from file
    try:
        with open('embeddings.pkl', 'rb') as f:
            return pickle.load(f)
    except EOFError:
        print("Error: The embeddings file is empty or not completely written.")
        return None
    except Exception as e:
        print(f"Error while loading embeddings from file: {e}")
        return None
    
    
    
device = "cuda:0" if torch.cuda.is_available() else "cpu"

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/msmarco-distilbert-base-v2")
model = AutoModel.from_pretrained("sentence-transformers/msmarco-distilbert-base-v2").to(device) 





print('\n----------------------------------------------------------------------------------- INDEX SETTINGS')
settings = client.indices.get_settings(index = index_name)
pp.pprint(settings)

print('\n----------------------------------------------------------------------------------- INDEX MAPPINGS')
mappings = client.indices.get_mapping(index = index_name)
pp.pprint(mappings)

print('\n----------------------------------------------------------------------------------- INDEX #DOCs')
print(client.count(index = index_name))








# Compute the query embedding
query = "drink"
query_emb = encode(query)

query_denc = {
  'size': 10,
#  '_source': ['doc_id', 'contents', 'sentence_embedding'],
#  '_source': ['doc_id', 'contents'],
  '_source': ['title', 'description', 'ingredients'],
   "query": {
        "knn": {
          "title_embedding": {
            "vector": query_emb[0].numpy(),
            "k": 2
          }
        }
      }
}

response = client.search(
    body = query_denc,
    index = index_name
)

print('\nSearch results:')
pp.pprint(response)



